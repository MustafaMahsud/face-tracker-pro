<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FaceTracker Pro</title>
  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    body {
      background-color: #0f172a;
      color: white;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      min-height: 100vh;
    }
    #tracker {
      position: relative;
      width: 90vw;
      max-width: 480px;
      aspect-ratio: 3/4;
      border-radius: 1rem;
      overflow: hidden;
      border: 2px solid rgba(255,255,255,0.15);
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
  </style>
</head>
<body class="p-4 space-y-6">

  <h1 class="text-3xl font-bold text-cyan-400">FaceTracker Pro</h1>
  <p class="text-slate-400">Realtime face circle — on-device tracking</p>

  <div id="tracker">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div class="flex gap-3">
    <button id="startBtn" class="bg-green-600 hover:bg-green-700 px-4 py-2 rounded-xl">Start Camera</button>
    <button id="stopBtn" class="bg-red-600 hover:bg-red-700 px-4 py-2 rounded-xl">Stop</button>
  </div>

  <script type="module">
    import { FilesetResolver, FaceLandmarker } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");
    let landmarker, stream, running = false;

    async function loadModel() {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
      );
      landmarker = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath:
            "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
        },
        runningMode: "VIDEO",
        numFaces: 1,
      });
      console.log("✅ Model ready");
    }

    async function startCamera() {
      stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      await new Promise(r => video.onloadedmetadata = r);
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      running = true;
      detectLoop();
    }

    async function detectLoop() {
      if (!running) return;
      const results = await landmarker.detectForVideo(video, Date.now());
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      if (results.faceLandmarks.length > 0) {
        const pts = results.faceLandmarks[0];
        const xs = pts.map(p => p.x * canvas.width);
        const ys = pts.map(p => p.y * canvas.height);
        const cx = xs.reduce((a,b)=>a+b,0)/xs.length;
        const cy = ys.reduce((a,b)=>a+b,0)/ys.length;
        const r = Math.max(
          (Math.max(...xs)-Math.min(...xs)),
          (Math.max(...ys)-Math.min(...ys))
        )/2;
        ctx.beginPath();
        ctx.strokeStyle = "#22d3ee";
        ctx.lineWidth = 4;
        ctx.arc(cx, cy, r, 0, 2*Math.PI);
        ctx.stroke();
      }
      requestAnimationFrame(detectLoop);
    }

    function stopCamera() {
      running = false;
      if (stream) stream.getTracks().forEach(t => t.stop());
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    }

    document.getElementById("startBtn").onclick = async () => {
      if (!landmarker) await loadModel();
      await startCamera();
    };
    document.getElementById("stopBtn").onclick = stopCamera;
  </script>
</body>
</html>
