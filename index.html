<script type="module">
  import {
    FaceDetector,
    FilesetResolver
  } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

  let video = document.getElementById("video");
  let canvas = document.getElementById("output");
  let ctx = canvas.getContext("2d");
  let detector;

  async function init() {
    const vision = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
    );
    detector = await FaceDetector.createFromOptions(vision, {
      baseOptions: {
        modelAssetPath:
          "https://storage.googleapis.com/mediapipe-models/face_detector/face_detector/float16/1/face_detector.task",
      },
      runningMode: "VIDEO",
    });

    startCamera();
  }

  async function startCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    video.onloadedmetadata = () => {
      video.play();
      detectFaces();
    };
  }

  async function detectFaces() {
    if (!detector) return requestAnimationFrame(detectFaces);

    const result = await detector.detectForVideo(video, Date.now());
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    if (result.detections.length > 0) {
      const box = result.detections[0].boundingBox;
      ctx.strokeStyle = "lime";
      ctx.lineWidth = 3;
      ctx.beginPath();
      ctx.arc(
        box.originX + box.width / 2,
        box.originY + box.height / 2,
        Math.max(box.width, box.height) / 2,
        0,
        2 * Math.PI
      );
      ctx.stroke();
    }

    requestAnimationFrame(detectFaces);
  }

  init();
</script>

